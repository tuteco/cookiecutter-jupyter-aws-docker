{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6910964-7464-49ff-94bb-54af3b0598f4",
   "metadata": {},
   "source": [
    "# Working with the AWS CLI\n",
    "\n",
    "to use the AWS CLI inside this container, you first need to authenticate on your host machine to get valid credentials.\n",
    "\n",
    "\n",
    "Once validated, check, that you have the correct credentials with the following command "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0818c5e5-95bf-4f2b-8ef0-46564633dd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws sts get-caller-identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d329a757-4267-47cc-abc9-84efd3898e16",
   "metadata": {},
   "source": [
    "If you get an error message \"An error occurred (ExpiredToken)\" then you need to re-authenticate on your host. the steps are\n",
    "- `cd`into your project root directory\n",
    "- locate your ECS-endpoint with `docker compose ps | grep ecs-local-endpoint`\n",
    "- restart the service with `docker compose restart <service_name>` \n",
    "\n",
    "Once successfully authenticated, check that your default region is correct. If not, please change the value in the `.env` file in your project root folder, and set `AWS_DEFAULT_REGION=<region>` to your desired region. The region you specify there is used as the default region for querying and creation / manipulation of resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5231d787-8d22-446e-afea-1334fc2a6f43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws configure list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb097f5-8505-4f6d-a142-63506aa08d26",
   "metadata": {},
   "source": [
    "## AWS S3 bucket creation\n",
    "\n",
    "Good practice, when you don't use any IaC solution like AWS Cloud Formation or Terraform, is to create the resources from JSON files. The project template is setup in a way, to use the `aws_objects` folder to store such JSON files.\n",
    "Main benefits of this approach are:\n",
    "- code used to create resources is reproducible\n",
    "- creating similar resources is easy, just copy a JSON and modify what is required\n",
    "\n",
    "First step is to create a JSON skeleton, that you can modify afterwards for your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba798e20-0caf-45a2-ad71-a098db21aeac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws s3api create-bucket --generate-cli-skeleton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886c13e8-f540-46de-9a48-909907632182",
   "metadata": {},
   "source": [
    "use this output and save it to `aws_objects/<bucket_name>.json`. To make it easier for the rest of the tutorial, we use an environment variable to store the S3 Buckets name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972f090c-4bfc-4a59-9d0d-1cd970a1e1a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%env S3_BUCKET=test.playground.cdwb.tecalliance.net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535f0763-1c48-4970-9725-3b8d55926ff6",
   "metadata": {},
   "source": [
    "Edit the JSON file and add the bucket name, ensure the ACL and LocationContraint  are set to your desired values.  \n",
    "If you need more information please read [AWS s3api create-bucket](https://docs.aws.amazon.com/cli/latest/reference/s3api/create-bucket.html)\n",
    "\n",
    "now its time to create the bucket from the JSOn file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4320b5-842e-4db3-b92b-6e6f3d8f7a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws s3api create-bucket --cli-input-json file://../aws_objects/$S3_BUCKET.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89de338d-ae80-49b4-903c-ce5a968c035f",
   "metadata": {},
   "source": [
    "If you do not want to share the data in your bucket with the public, you also need to ensure, that public access is blocked. This time without JSON input, just used the command line only for demonstration purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575987ff-3dc9-47c5-a192-cea8e81c8de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws s3api put-public-access-block \\\n",
    "    --bucket $S3_BUCKET \\\n",
    "    --public-access-block-configuration \"BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a527048d-e48d-4901-b046-ca0d6857a0b1",
   "metadata": {},
   "source": [
    "## AWS S3 upload\n",
    "\n",
    "Let's generate some fake data ans save it to a local file that we can upload later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73df1d9a-cef6-43f1-8176-f74c2f3ee4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint, seed\n",
    "\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "from faker_music import MusicProvider\n",
    "\n",
    "# initialize the fake factory\n",
    "fake = Faker(\"it_IT\")\n",
    "fake.add_provider(MusicProvider)\n",
    "\n",
    "data = []\n",
    "\n",
    "# generate a couple of rows fake data\n",
    "for i in range(randint(750, 1500)):\n",
    "    data.append(\n",
    "        [\n",
    "            fake.name(),\n",
    "            fake.music_genre(),\n",
    "            fake.music_instrument(),\n",
    "            fake.city(),\n",
    "            fake.state(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# define a dataframe with the columns\n",
    "df = pd.DataFrame(data, columns=[\"name\", \"genre\", \"instrument\", \"city\", \"state\"])\n",
    "\n",
    "# and sore it as csv\n",
    "df.to_csv(\n",
    "    \"../data/italian_musicans.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3191fe7-a214-4585-be3e-5618419f8c22",
   "metadata": {},
   "source": [
    "After the file is created, check the content by navigating in the notebook file browser to the generated file and double click it.\n",
    "\n",
    "Now we will upload the file to the previously created S3 bucket. We will use [AWS CLI S3 high level commands](https://docs.aws.amazon.com/cli/latest/userguide/cli-services-s3-commands.html) instead of the s3api calls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f167808d-2419-4705-b9be-6ef4685ab1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws s3 cp ../data/italian_musicans.csv s3://$S3_BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedbce1b-6e6a-4e34-bdb4-569f0411dbb1",
   "metadata": {},
   "source": [
    "## AWS S3 discover\n",
    "\n",
    "First is to get an overview of all S3 buckets with the associated tags. As there is no build in AWS CLI functionality, a small script is required. Please be patient: depending on the number of buckets in your account it can take some time for the result to display  \n",
    "\n",
    "The [jq](https://stedolan.github.io/jq/) tool is used to parse the JSON output of the AWS CLI. \n",
    "Also [tr](https://en.wikipedia.org/wiki/Tr_(Unix)) is used to replace newline character with a tab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2752506-9bd3-42e3-bdbe-5fcf8fc9dcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for BUCKET in $(aws s3api list-buckets | jq .Buckets[].Name -r); do\n",
    "    RESULT=$(aws s3api get-bucket-tagging --bucket $BUCKET 2>&1)\n",
    "    if [[ $RESULT =~ \"(NoSuchTagSet)\" ]]; then\n",
    "        echo $BUCKET\n",
    "    else\n",
    "        tags=$(echo $RESULT |jq -c '.[][] | {(.Key): .Value}' | tr '\\n' '\\t')\n",
    "        echo $BUCKET '|' $tags\n",
    "    fi\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09efe0f6-bb32-449b-96fb-857d3c3f6f6e",
   "metadata": {},
   "source": [
    "And finally we query the contents of our created S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b11787-eb31-415f-8ae4-88b5645bfb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws s3 ls s3://$S3_BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc563a3-a155-44db-b3ab-f5378aa2ceba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
